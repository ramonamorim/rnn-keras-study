{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import json\n",
    "import spacy\n",
    "import re\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers import Embedding, LSTM, Dense, Bidirectional, Dropout\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import pad_sequences\n",
    "from keras.callbacks import CSVLogger, TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "#All these words you can modify. I used them for my specific problem during the study\n",
    "\n",
    "DAYS = ['mon', 'tue', 'wed', 'thu', 'fri', 'sat', 'sun', 'monday', 'tuesday', 'wednesday', 'thursday', 'friday', 'saturday', 'sunday']\n",
    "MONTHS = ['january', 'february', 'march', 'april', 'may', 'june', 'july', 'august', 'september', \n",
    "        'october', 'november', 'december', 'ja', 'jan', 'fe', 'feb', 'march', 'mar',  'ma', 'ap',\n",
    "        'apr', 'jun', 'ju', 'jul', 'aug', 'au', 'sept', 'sep', 'se', 'oct', 'nov', 'decembe', 'dec']\n",
    "SUFIX = [\"st\", \"nd\", \"rd\", \"th\"]\n",
    "TIME = ['hr', 'hrs', 'hour', 'hours', 'date', 'gmt', 'time']\n",
    "MEASUREMENT_UNITS = ['mt', 'lt', 'foot', 'lat']\n",
    "\n",
    "for word in DAYS + MONTHS + SUFIX + TIME + MEASUREMENT_UNITS:\n",
    "    nlp.vocab[word].is_stop = True\n",
    "\n",
    "stop_words = nlp.Defaults.stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stop_words(text):\n",
    "    doc = nlp(text)  \n",
    "    tokens_without_stopwords = [token.text for token in doc if not token.is_stop]  \n",
    "    return \" \".join(tokens_without_stopwords)  \n",
    "\n",
    "def remove_numbers_caracters(text):\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    regex = r'[^a-zA-Z\\s]'\n",
    "    text = re.sub(regex, ' ', text)\n",
    "    text = text.lower() \n",
    "    return text\n",
    "\n",
    "def correct_white_spaces(text):\n",
    "    text_fixed = re.sub(r'\\s+', ' ', text)\n",
    "    return text_fixed\n",
    "\n",
    "#used to clean the data not obligatory apply this function\n",
    "def remove_data_by_key(data):\n",
    "    data_no_ev = list(filter(lambda item: item['key'] != 'example_key', data))\n",
    "    return data_no_ev\n",
    "\n",
    "#not used. i tried to use for some improvement \n",
    "def lemmatize_text(text):\n",
    "    doc = nlp(text)\n",
    "    lemmatized_tokens = [token.lemma_ for token in doc if not token.is_stop]\n",
    "    return \" \".join(lemmatized_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/data/output.json', encoding='utf-8') as file:\n",
    "    json_data = json.load(file)\n",
    "\n",
    "data = json_data \n",
    "\n",
    "for item in data:\n",
    "    \n",
    "    item['lineText'] = remove_numbers_caracters(item['lineText']).strip()\n",
    "    item['lineText'] = remove_stop_words(item['lineText']).strip()\n",
    "    item['lineText'] = correct_white_spaces(item['lineText']).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open('/data/output.json', 'w') as arquivo:\n",
    "    json.dump(data, arquivo, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/data/clean.json', encoding='utf-8') as file:\n",
    "    json_data = json.load(file)\n",
    "\n",
    "data = None\n",
    "print(data)\n",
    "data = json_data    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(data)\n",
    "train_data = data[:int(0.8 * len(data))]\n",
    "test_data = data[int(0.8 * len(data)):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_texts = [item['lineText'] for item in train_data]\n",
    "test_texts  = [item['lineText'] for item in test_data]\n",
    "\n",
    "# Separate the labels of dataset\n",
    "train_labels = [item['key'] for item in train_data]\n",
    "test_labels = [item['key'] for item in test_data]\n",
    "\n",
    "# Unifying all dataset labels without duplication\n",
    "all_labels_unique = set(train_labels).union(set(test_labels))\n",
    "\n",
    "class_to_index = {cls: i for i, cls in enumerate(all_labels_unique)}\n",
    "train_label_indices = [class_to_index[cls] for cls in train_labels]\n",
    "test_label_indices = [class_to_index[cls] for cls in test_labels]\n",
    "\n",
    "num_classes = len(all_labels_unique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(train_texts)\n",
    "train_sequences = tokenizer.texts_to_sequences(train_texts)\n",
    "\n",
    "max_sequence_length = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########CALLBACKS##############\n",
    "csv_logger = CSVLogger('training_final.log')\n",
    "tbCallBack = TensorBoard(log_dir='./tensorboard_final', histogram_freq=0, write_graph=True, write_grads=True, write_images=False, embeddings_freq=0, embeddings_layer_names=None, embeddings_metadata=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_1 (Embedding)     (None, 100, 200)          842400    \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 200)               320800    \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 256)               51456     \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 256)               0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 191)               49087     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,263,743\n",
      "Trainable params: 1,263,743\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 100, input_length=max_sequence_length))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "191"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sequences_padded = pad_sequences(train_sequences, maxlen=max_sequence_length)\n",
    "train_labels_onehot = np.zeros((len(train_label_indices), num_classes))\n",
    "for i, index in enumerate(train_label_indices):\n",
    "    train_labels_onehot[i, index] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(train_sequences_padded, train_labels_onehot, epochs=100, batch_size=64, callbacks=[csv_logger, tbCallBack], use_multiprocessing=True)\n",
    "model.save('model_example.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sequences = tokenizer.texts_to_sequences(test_texts)\n",
    "\n",
    "test_sequences_padded = pad_sequences(test_sequences, maxlen=max_sequence_length)\n",
    "\n",
    "test_labels_onehot = np.zeros((len(test_label_indices), num_classes))\n",
    "for i, indices in enumerate(test_label_indices):\n",
    "    test_labels_onehot[i, indices] = 1\n",
    "\n",
    "loss, accuracy = model.evaluate(test_sequences_padded, test_labels_onehot)\n",
    "print('Test Loss:', loss)\n",
    "print('Test Accuracy:', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "model = load_model('model_example.h5')\n",
    "\n",
    "new_texts = ['here you can insert your phrase to test the resulted model with the phrase and check the result'] \n",
    "new_sequences = tokenizer.texts_to_sequences(new_texts)\n",
    "new_sequences_padded = pad_sequences(new_sequences, maxlen=max_sequence_length)\n",
    "\n",
    "predictions = model.predict(new_sequences_padded)\n",
    "\n",
    "teste = []\n",
    "\n",
    "for x in all_labels_unique:\n",
    "    teste.append(x)\n",
    "\n",
    "predicted_labels = [teste[np.argmax(pred)] for pred in predictions]\n",
    "\n",
    "prob_max = predictions.max()\n",
    "\n",
    "for pred, text, label in zip(predictions, new_texts, predicted_labels):\n",
    "    print('Text:', text)\n",
    "    print('Predicted label:', label)\n",
    "    print('higher probability:', pred.max())        \n",
    "    print('---')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
